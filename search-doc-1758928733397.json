[{"title":"File Creation","type":0,"sectionRef":"#","url":"/vrs/docs/FileCreation","content":"","keywords":"","version":"Next"},{"title":"Choosing a RecordableTypeId​","type":1,"pageTitle":"File Creation","url":"/vrs/docs/FileCreation#choosing-a-recordabletypeid","content":" Originally, each device was given its own RecordableTypeId enum value, which implied the modification of the &lt;vrs/StreamId.h&gt; file every time.  We now recommend that you use an existing «Recordable Class» enum value of RecordableTypeId enum, defined in &lt;vrs/StreamId.h&gt;. Choose the one that best matches your use case, and associate it with a «Recordable Flavor», which is a text string describing your specific use case.  «Recordable Flavors» should respect the following conventions, that will help make «flavors» unique, and more importantly, make them meaningful to human readers.  «Flavors» should preferably start with “team”, “project”, “device”, or “tech”.Follow it with a name for that team, project, device, or tech.Continue with a secondary description, and even a third description, as desired.Separate the different sections using a '/' character.  Flavor examples/suggestions (not actually used):  tech/positional_tracking, if the recordable type ID is enough.tech/positional_tracking/top_left_camera to differentiate different instances of the same recordable types.device/aria, or maybe device/aria/positional_camera/top_leftproject/quest/groundtruth  A lot of discretion exists when choosing a flavor, so use your best judgment. ","version":"Next","tagName":"h2"},{"title":"File Playback","type":0,"sectionRef":"#","url":"/vrs/docs/FilePlayback","content":"","keywords":"","version":"Next"},{"title":"Why are records read using a StreamPlayer callback object, rather than a \"regular\" blocking call?​","type":1,"pageTitle":"File Playback","url":"/vrs/docs/FilePlayback#why-are-records-read-using-a-streamplayer-callback-object-rather-than-a-regular-blocking-call","content":" New users of VRS are sometimes surprised that they cannot &quot;just read&quot; a record to retrieve an image or some other parts of a record using a blocking call, which would feel more convenient to them. Instead, they are forced to use StreamPlayer callback objects that feel cumbersome and slow. Why is that?  The reasons are flexibility and performance.  If the data in a record had to be returned by a &quot;classic&quot; API, you would not be able to specify how the record needs to be interpreted. When using callback StreamPlayer objects, the interpretation of the data is delegated to an object (therefore, code) that can implement the exact method you choose to interpret a particular type of record, and it can decide which part(s) need to be read, and how and where the data needs to be saved. This should help to avoid unnecessary memory copy, which is particularly important with image buffers.  Maybe you want to interpret the data using the RecordFormat conventions with a RecordFormatStreamPlayer, but maybe not. Maybe you need video codec support, which will probably force a dependency on ffmpeg, but maybe not, and maybe you prefer to pass a video codec buffer to an HW decoder yourself.  StreamPlayer objects are not asynchronous callback objects, nor are they some kind of &quot;futures&quot;. After a readRecord call returns, the record has been read and all the callbacks have been completed. No latency is introduced in that process - on the contrary. Since the record's data interpretation is delegated to code that knows best what to do with the record's data, latencies are reduced to a minimum. For instance, the data layout part of a record might be read, while the image data that follows could easily be skipped. ","version":"Next","tagName":"h3"},{"title":"The FileHandler Interface","type":0,"sectionRef":"#","url":"/vrs/docs/FileHandler","content":"","keywords":"","version":"Next"},{"title":"Why is FileHandler useful?​","type":1,"pageTitle":"The FileHandler Interface","url":"/vrs/docs/FileHandler#why-is-filehandler-useful","content":" VRS files are not necessarily a single file on a hard drive. They can be any of the following types of files:  Chunked files on one or more local hard drivesA binary blob in cloud storageA list of binary blobs in cloud storage (chunks in cloud storage)Some kind of database reference that points to any of the above...  ","version":"Next","tagName":"h2"},{"title":"What does FileHandler do?​","type":1,"pageTitle":"The FileHandler Interface","url":"/vrs/docs/FileHandler#what-does-filehandler-do","content":" The FileHandler interface enables you to do the following read operations with a file:  Open the file with a given path (a text string to interpret, details are below)Provide the file sizeMove the read positionRead a number of bytesProvide caching hintsClose the file and release the allocated resources (such as file system handles and caches)  At the core, that is all FileHandler does.  ","version":"Next","tagName":"h2"},{"title":"File Paths, URIs, and JSON File Specifications​","type":1,"pageTitle":"The FileHandler Interface","url":"/vrs/docs/FileHandler#file-paths-uris-and-json-file-specifications","content":" The primary way to identify a file is to provide a file path, but that is insufficient.  We need to be able to handle the following use cases:  Chunked filesFiles in cloud storageChunked files in cloud storageReferences to a database entry, pointing to a file in another storage locationFiles stored in Amazon AWS S3, Microsoft Azure, Google Cloud Storage, etc.  We want to avoid making special-case implementations of our APIs and keep them simple, so using a single string to represent any path is a requirement. URIs are powerful and convenient, but chunked files quickly become difficult to represent, as chunked files can be a collection of URIs. For extreme cases, JSON appears to be the only reasonable option.  In the end, VRS supports regular file paths, URIs, and JSON file specifications, as transparently as possible.  ","version":"Next","tagName":"h2"},{"title":"JSON File Specifications​","type":1,"pageTitle":"The FileHandler Interface","url":"/vrs/docs/FileHandler#json-file-specifications","content":" A JSON file specification must have the following required fields:  “storage&quot;: the name of the FileHandler able to read that location.“chunks”: an ordered list of strings, each representing a chunk of the logical file, in a format that the designated FileHandler can handle.  A valid JSON file specification can simply be as follows: {&quot;storage&quot;:&quot;diskfile&quot;,&quot;chunks&quot;:[&quot;/local/folder/file.vrs&quot;]}  A chunked local file may look like this: {&quot;storage&quot;:&quot;diskfile&quot;,&quot;chunks&quot;:[&quot;/local/folder/file.vrs&quot;,&quot;/local/folder/file.vrs_part_2&quot;]}  A chunked file in the cloud might be accessed using chunks published as HTTP objects, and be represented like this: {&quot;storage&quot;:&quot;http&quot;,&quot;chunks&quot;:[&quot;http://cdn.meta.com/HASH1&quot;,&quot;http://cdn.meta.com/HASH2&quot;]}  Optional Fields  When dealing with objects in cloud storage, it can be expensive to do a basic operation, such as getting the size of a file chunk, particularly when you have many. Knowing how to name a file to download, and remembering how a file was originally referenced is critical. Therefore, the following optional fields have been introduced to answer these questions:  chunk_sizes: an ordered list of integers, which should match the list of chunks.filename: a name suitable to save the file locally. It might be the name of the file before it was uploaded to cloud storage.source_uri: a URI representation of how the object was initially represented, in particular if the JSON file specification was generated from a URI.  Example: {&quot;storage&quot;:&quot;http&quot;,&quot;chunks&quot;:[&quot;http://cdn.meta.com/HASH1&quot;],&quot;chunk_sizes&quot;:[123456],&quot;filename&quot;:&quot;scene23.vrs&quot;,&quot;source_uri&quot;:&quot;aria:456789&quot;}  Extra fields  Additional fields can be provided, outside of the core JSON file specifications. Extra values are string values, which name isn't reserved for required or optional fields. These might include authentication options for some cloud storage implementations, for instance: {&quot;storage&quot;:&quot;http&quot;,&quot;chunks&quot;:[&quot;http://cdn.meta.com/HASH1&quot;],&quot;source_uri&quot;:&quot;aria:456789&quot;,&quot;auth_token&quot;:&quot;09*JOYBAaSLBG@#O@&quot;}  ","version":"Next","tagName":"h3"},{"title":"URIs​","type":1,"pageTitle":"The FileHandler Interface","url":"/vrs/docs/FileHandler#uris","content":" URIs can be very compact and convenient, and they are commonly used already, so FileHandler can also handle URIs. VRS interprets the scheme part of the URI as a FileHandler name, and uses a FileHandler with that name to interpret the rest of the URI. Example: aria:456789?auth_token=09%2AJOYBAaSLBG%40%23O%40  ","version":"Next","tagName":"h3"},{"title":"FileSpec​","type":1,"pageTitle":"The FileHandler Interface","url":"/vrs/docs/FileHandler#filespec","content":" Parsing JSON messages and URIs can become expensive if we need to repeat the operation multiple times. This is why, internally, string paths are always immediately converted into FileSpec objects, which are used for all file location operations.  At its core, a FileSpec object is simply a struct with the following public fields:   string fileHandlerName; string fileName; string uri; vector&lt;string&gt; chunks; vector&lt;int64_t&gt; chunkSizes; map&lt;string, string&gt; extras;   FileSpec provides helper functions to convert to and from JSON and to get the size of a file (when specified directly). In practice, JSON file specifications and FileSpec objects are equivalent.  ","version":"Next","tagName":"h3"},{"title":"FileHandlerFactory​","type":1,"pageTitle":"The FileHandler Interface","url":"/vrs/docs/FileHandler#filehandlerfactory","content":" The primary role of FileHandler is to provide an abstract way to read files. VRS core provides a single FileHandler implementation that can read local files. All other FileHandler implementations do not belong in the VRS core. This is so that its dependencies can be limited as much as possible, and it can be easily compiled for mobile and other embedded environments.  The FileHandlerFactory singleton allows:  Registering additional FileHandler implementationsRequesting the construction of FileHandler objects by nameInterpreting a file represented by a local file path, a URI, or a JSON file specification using the appropriate FileHandler.  FileHandlerFactory is a pretty small but essential class, used to open files as follows:  Convert a string path into a FileSpecCreate a FileHandler instance specified by name in the FileSpec objectRequest a FileHandler instance to actually open the FileSpec, whatever that means for that FileHandlerReturn an error code and the FileHandler object  All this is done by the FileHandlerFactory API as shown below:  int FileHandlerFactory::delegateOpen( const string&amp; path, unique_ptr&lt;FileHandler&gt;&amp; outNewDelegate);   When the VRS RecordFileReader class opens a VRS file using the string path, the FileHandlerFactory API finds the FileHandler that is needed for all read operations. The RecordFileReader object does not need to know if the VRS file is local, chunked, or in cloud storage.  ","version":"Next","tagName":"h2"},{"title":"Further Nested Delegation​","type":1,"pageTitle":"The FileHandler Interface","url":"/vrs/docs/FileHandler#further-nested-delegation","content":" For completeness, more levels of indirection might be required. When FileHandlerFactory asks FileHandler to open a file, it uses the FileHandler eponym API to open the FileSpec, using delegateOpen. This is so the FileHandler itself may delegate the actual handling of the FileSpec to yet another FileHandler implementation.  For example, imagine you have an HTTP FileHandler, which can stream data from an HTTP file. Also imagine you have a database of VRS files in cloud storage, named &quot;MyAIDataSet&quot;, which references files using a 64 bit number. You can implement a custom MyAIDataSetFileHandler, which accesses the MyAIDataSet database, and converts MyAIDataSet URIs into HTTP URLs. The MyAIDataSetFileHandler code will convert the URI, &quot;myaidataset:12345&quot;, for the MyAIDataSet dataset #12345, into a content delivery network (CDN) URL, appropriate for your computer (it might do that by looking up the dataset online using a service that will return that URL after validations and permissions checks). Then, the MyAIDataSetFileHandler logic will delegate reading the data from that URL to the HTTP FileHandler that can stream the data from an HTTP URL. The RecordFileReader will decode the remove VRS file just as if it were a local file, because all its file accesses are done using a FileHandler instance.  ","version":"Next","tagName":"h3"},{"title":"Interpreting Strings as JSON File Specifications, URIs, or File Paths​","type":1,"pageTitle":"The FileHandler Interface","url":"/vrs/docs/FileHandler#interpreting-strings-as-json-file-specifications-uris-or-file-paths","content":" Putting it all together, when VRS needs to interpret a string path, the following logic is used:  If the string path is a JSON specification, it is converted into a FileSpec object with a FileHandler name.If the string path is a URI, it is parsed and made into a valid FileSpec object, by the FileHandler with the same name as the URI scheme, if such a FileHandler is available.Otherwise, the string path is assumed to be a local file path, that is readable to the VRS built-in disk file FileHandler, using the standard POSIX file APIs.Once the FileSpec object is built, the FileHandlerFactory tries to open the file, using delegateOpen() to find the correct FileHandler for the job.  ","version":"Next","tagName":"h2"},{"title":"FileHandler vs. WriteFileHandler​","type":1,"pageTitle":"The FileHandler Interface","url":"/vrs/docs/FileHandler#filehandler-vs-writefilehandler","content":" As you may have noticed, FileHandler is strictly a read-only interface, because most FileHandler implementations, by far, only support reading. This is largely because cloud storage is usually either completely immutable, or only offers very limited creation and mutation options. Some cloud storage might only support creating new objects. Other cloud storage might only support creating new objects of constrained size, but will be able to concatenate existing objects to create new objects.  The WriteFileHandler classes that derive from FileHandler can add support for write operations, but they may not be able to support any write operations. Creating a WriteFileHandler is much more complicated than creating a read-only FileHandler.  ","version":"Next","tagName":"h2"},{"title":"Reading is easy​","type":1,"pageTitle":"The FileHandler Interface","url":"/vrs/docs/FileHandler#reading-is-easy","content":" A (read) FileHandler only really needs to implement the following operations:  open()close()getFileSize()setPos(position)read(length)  Effectively, cloud storage is typically stateless, and only offers support for getFileSize() and readRange(position, length), but these can be enough to implement the FileHandler interface. As a result, it is easy to read cloud-stored data using the same interface as for files, even if performance will obviously differ greatly. In practice, all our FileHandler interfaces fully implement the read operations, or delegate them to other FileHandler implementations, that implement caching too.  ","version":"Next","tagName":"h3"},{"title":"Writing is complex​","type":1,"pageTitle":"The FileHandler Interface","url":"/vrs/docs/FileHandler#writing-is-complex","content":" You can do the following file operations with typical disk file APIs:  Create a file at a location, so that an entry appears in the file system, immediately.Write bytes to the file and extend the file.Write more bytes to the file, and extend the file further.Handle write requests of any size (OS system caching minimizes performance issues for you).Seek to a past location, to read what is written before.Overwrite or extend a file. (You cannot insert bytes into the middle of a file.)Open an existing file to modify it.If your app crashes or does not close the file explicitly, the data may not be fully written to disk, depending on the implementation, so it is common to have partially written files.  In contrast, when writing files to a cloud storage, things typically happen as follows:  Files are not actually created until the final close/commit/submit/finalize operation is performed, after a series of successful write operations.Write operations are network operations. They have very high latencies and high error rates. Retrying file write operations is often required.Small write operations can be extremely inefficient on the backend, in terms of storage. So, writing data in large chunks makes a huge difference.You can not read back data you have just written, until you have finalized the object, and it is fully committed.You can not overwrite or modify data you have already written.If an app crashes before the upload is finalized, anything uploaded is probably lost. Some cloud storage solutions can recover partial uploads or chunks, but at the cost of added code complexity.  In practice, all these constraints vary greatly between cloud storage solutions. Even if you only need to upload existing files, as they are, to a cloud storage, the cloud storage may have a maximum file size, or a chunking preference. So, if you want to store large files in the cloud, you have to chunk them and manage the chunks yourself to a certain extent.  ","version":"Next","tagName":"h3"},{"title":"Do not use WriteFileHandler implementations directly​","type":1,"pageTitle":"The FileHandler Interface","url":"/vrs/docs/FileHandler#do-not-use-writefilehandler-implementations-directly","content":" Cloud storage write operations do not provide the same type of flexibility as file APIs. Consequently, it is usually not be possible to provide a generic WriteFileHandler implementation - one that will work for all use cases. While any FileHandler implementation can safely be used to access any type of file anywhere, you cannot think of WriteFileHandler as something that will work for all use cases. ","version":"Next","tagName":"h3"},{"title":"File Structure","type":0,"sectionRef":"#","url":"/vrs/docs/FileStructure","content":"","keywords":"","version":"Next"},{"title":"File Tags​","type":1,"pageTitle":"File Structure","url":"/vrs/docs/FileStructure#file-tags","content":" VRS files contain file tags. File tags are a set of name/value pairs, both of which are text strings. File tags may be set to any text string, but VRS provides some tag conventions to represent a few common concepts in VRS.  ","version":"Next","tagName":"h2"},{"title":"Streams​","type":1,"pageTitle":"File Structure","url":"/vrs/docs/FileStructure#streams","content":" VRS files contain multiple streams, each associated with a device type, defined by a RecordableTypeId enum value.  A subset of RecordableTypeId values represent generic devices, which we called «Recordable Class» IDs. «Recordable Class» IDs must be paired with «Recordable Flavor» values to identify a user-specific type of device or data contained in that stream.  A device type can also represent a virtual device, such as the output of a vision algorithm, or an event, such as a keyboard, mouse, or orientation event, that can be used to exercise code.  ","version":"Next","tagName":"h2"},{"title":"Stream Tags​","type":1,"pageTitle":"File Structure","url":"/vrs/docs/FileStructure#stream-tags","content":" Streams also have tags, which are also a set of name/value pairs, both of which are text strings. Stream tags may be set to any string value, but VRS provides some tag conventions for streams to represent a few common concepts.  ","version":"Next","tagName":"h2"},{"title":"Records​","type":1,"pageTitle":"File Structure","url":"/vrs/docs/FileStructure#records","content":" Streams contain a time-sorted sequence of records. Records have the following metadata:  TimestampStreamIdRecord TypeRecord Format Version Timestamps are double type values that count the number of seconds from a point in time, such as EPOCH, boot time, or any other fixed point. note All records in a VRS file must use timestamps in the same time domain. VRS sorts the records in the file and in playback in that order. If your data has device-specific timestamps you need to keep, you must save those timestamps inside the records. Images and IMU samples may have their own device-specific timestamps. Timestamps are double type values, to be compatible with how time is represented in the Oculus world. Frequently, devices such as cameras and IMUs use their own internal clocks or counters, which are not synchronized. Using the double type can now help distinguish VRS timestamps from device specific timestamps. caution When recording audio, be aware that audio devices are de facto clocks; they produce data at a specific sample rate. If you use a system clock for your VRS records, that clock and your audio clock will drift. This will make it difficult to accurately synchronize audio data with your other data, especially images. It’s reasonably easy to establish a correspondence between your system clock and the first audio sample when you start recording. After that, the count of the audio samples produced, divided by the sample rate, determines the actual duration of time as seen by the audio device, and that duration will not match the duration of time measured by your system clock. Ten minutes after that, your system clock and the audio clock will no longer be in sync and the gap will be visually noticeable. Therefore, be extremely careful when trying to find which audio sample corresponds to a particular image. This is very challenging to accomplish and is seldom very accurate. ","version":"Next","tagName":"h2"},{"title":"Organization","type":0,"sectionRef":"#","url":"/vrs/docs/Organization","content":"Organization The VRS documentation has different parts: This documentation, which describes the concepts, features, and principles of VRS.The API Documentation, generated using Doxygen. To generate the API documentation from the VRS code, run: cd &lt;vrs_repo_top_level_folder&gt; doxygen vrs/Doxyfile You'll find the API documentation in html at website/static/doxygen/index.html.Sample code, which is not functional, but demonstrates how to use the APIs. SampleRecordAndPlay.cpp demonstrates different ways to create a file, by dumping the whole content from memory to disk after creating all the records in memory, or by writing the record to disk while continuing to create records.SampleImageReader.cpp demonstrates how to read typical image records, that is, records containing metadata and an image.SampleRecordFormatDataLayout.cpp demonstrates how to read metadata blocks. Sample apps, which are runnable apps (though not actually useful). The first app generates a VRS file with different record types containing made-up data. The second app reads that VRS file and verifies that the record content is as expected.","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/vrs/docs/Overview","content":"","keywords":"","version":"Next"},{"title":"Appropriate Use Cases​","type":1,"pageTitle":"Overview","url":"/vrs/docs/Overview#appropriate-use-cases","content":" VRS is designed to record similar looking bundles of data produced repeatedly over a period of time, in time-stamped records.  Good Use CasesPoor Use Cases VRS is very well suited to record and playback: Data produced by the cameras and sensors of a Meta Quest device, including IMUs.Data produced by the cameras and sensors of Project Aria Glasses, including positional tracking cameras, eye tracking cameras, barometer sensors, GPS and BT beacons, multi-channel audio.Data produced in burst of activity, such as keyboard or mouse input data.TCP/IP or USB packets.  info While VRS is very effective at streaming very large amounts of data to disk, potentially to and from cloud storage, with real-time lossless compression, creating files potentially very large (typical VRS files range from 5 to 80 GB, but 1.5 TB VRS files exist), editing VRS files is not nearly as convenient, as the entire container typically needs to be rewritten.  ","version":"Next","tagName":"h2"},{"title":"Data Types and Data Conventions​","type":1,"pageTitle":"Overview","url":"/vrs/docs/Overview#data-types-and-data-conventions","content":" VRS provides standardized methods to store images, audio, and discrete sensors data in compact and format evolution resilient records, so you can save data without having to worry too much about evolving requirements. But while VRS standardizes how to save common data types, VRS does not prescribe how to address specific use cases. Data format conventions are desirable, to enable teams working on identical or similar use cases to exchange data. However, such data format conventions are out of the scope of VRS.  ","version":"Next","tagName":"h2"},{"title":"Features Overview​","type":1,"pageTitle":"Overview","url":"/vrs/docs/Overview#features-overview","content":" VRS files contain multiple streams of time-sorted records generated by a set of sensors (camera, IMU, thermometer, GPS, etc), typically one set of sensors per stream.The file and each stream contain an independent set of tags, which are string name/value pairs that describe them.Streams may contain Configuration, State and Data records, each with a timestamp in a common time domain for the whole file. Typically, streams contain one Configuration record and one State record, followed by one to millions of Data records.Records are structured as a succession of typed content blocks. Typically, content blocks are metadata, image, audio, and custom content blocks.Metadata content blocks contain raw sensor data described once per stream, making the file format very efficient. The marginal cost of adding 1 byte of data to each metadata content block in a stream is 1 byte per record (or less when lossless compression happens).Records can be losslessly compressed using lz4 or zstd, which can be fast enough to compress while recording on device.Multiple threads can create records concurrently for the same file, without CPU contention.VRS supports huge file sizes (tested with multi-terabyte use cases).VRS supports chunked files: auto-chunking on creation and automated chunk detection for playback.Playback is optimized for timestamp order, which is key for network streaming.Random-access playback is supported.Custom FileHandler implementations can add support for cloud storage streaming. ","version":"Next","tagName":"h2"},{"title":"Image Support","type":0,"sectionRef":"#","url":"/vrs/docs/ImageSupport","content":"","keywords":"","version":"Next"},{"title":"Additional Image Properties​","type":1,"pageTitle":"Image Support","url":"/vrs/docs/ImageSupport#additional-image-properties","content":" To be complete, image content blocks, in particular image/raw content blocks, may require the following complementary properties for an image:  ResolutionStridePixel format  Without these properties, image/raw images cannot be interpreted safely. These properties can be provided in two distinct ways:  The image properties can be part of the RecordFormat description itself, meaning that the records using that RecordFormat will all be using the exact same pixel format. The content block description might look like this: image/raw/640x480/pixel=grey8image/raw/640x480/pixel=yuv_i420_split/stride=640 The image properties can be provided in the datalayout content block found either before the image content block in the same record, or in the stream’s last configuration record. When using this method, the properties must be specified using the Datalayout Conventions, which are merely a collection of data types and labels to use in your DataLayout to specify properties.  Whichever method you use, VRS collects the image properties for you, so that the RecordFormatStreamPlayer::onImageRead() callback can directly provide a content block object that you can query for the image format, regardless of how or where it was specified.  ","version":"Next","tagName":"h2"},{"title":"Specifying Additional Image Properties​","type":1,"pageTitle":"Image Support","url":"/vrs/docs/ImageSupport#specifying-additional-image-properties","content":" For image/raw and image/video images, use a vrs::DataLayoutConventions::ImageSpec to describe your image in a configuration record, or in a DataLayout block just before the image content block of your data records.  The DataLayout fields are:   DataPieceValue&lt;ImageSpecType&gt; width{kImageWidth}; DataPieceValue&lt;ImageSpecType&gt; height{kImageHeight}; DataPieceValue&lt;ImageSpecType&gt; stride{kImageStride}; DataPieceValue&lt;ImageSpecType&gt; stride2{kImageStride2}; DataPieceEnum&lt;PixelFormat, ImageSpecType&gt; pixelFormat{kImagePixelFormat}; // For image/custom_codec and image/video only DataPieceString codecName{kImageCodecName}; // required DataPieceValue&lt;ImageSpecType&gt; codecQuality{kImageCodecQuality}; // optional   For image/video images, specify the keyframe timestamp and the keyframe index, using the following DataLayout fields:   DataPieceValue&lt;double&gt; keyFrameTimestamp{kImageKeyFrameTimeStamp}; DataPieceValue&lt;ImageSpecType&gt; keyFrameIndex{kImageKeyFrameIndex};   ","version":"Next","tagName":"h3"},{"title":"Image Format Differences​","type":1,"pageTitle":"Image Support","url":"/vrs/docs/ImageSupport#image-format-differences","content":" ","version":"Next","tagName":"h2"},{"title":"image/raw​","type":1,"pageTitle":"Image Support","url":"/vrs/docs/ImageSupport#imageraw","content":" Without the following properties, image/raw image content blocks can not be interpreted:  Required properties: width, height, pixel format.Optional properties: stride, stride2.  When properties are provided using the Datalayout Conventions, all additional properties must be provided by a single DataLayout structure in one record. For example, you must not put the pixel format in the configuration record, and then put the image dimensions in the data record. You must either put both the pixel format and the image dimensions in a configuration record's DataLayout, or you must put them both in a DataLayout just before the image content block.  ","version":"Next","tagName":"h3"},{"title":"image/png, image/jpg and image/jxl​","type":1,"pageTitle":"Image Support","url":"/vrs/docs/ImageSupport#imagepng-imagejpg-and-imagejxl","content":" png, jpg, and jxl payloads are exactly the same as png, jpg, and jxl files, which are fully self-described. Therefore, when those image formats are used, properties specified using the Datalayout Conventions are ignored by VRS.  ","version":"Next","tagName":"h3"},{"title":"image/custom_codec​","type":1,"pageTitle":"Image Support","url":"/vrs/docs/ImageSupport#imagecustom_codec","content":" Required properties: codec name.Optional properties: width, height, pixel format, codec name, keyframe timestamp, keyframe index.  The only property VRS itself requires is a codec name for an image content block to be recognized. However, a particular custom codec may require additional properties, such as width, height, and pixel format. Whether a custom codec requires that information or not an implementation detail of each specific custom codec. VRS does not support any particular custom codec by default, they are meant for experimentations and special cases.  ","version":"Next","tagName":"h3"},{"title":"image/video​","type":1,"pageTitle":"Image Support","url":"/vrs/docs/ImageSupport#imagevideo","content":" Video image content blocks require additional properties to decode images:  Required properties: width, height, pixel format, codec name, keyframe timestamp, keyframe index.Optional properties: compression quality, which may affect how to decode the image.  Similar to image/raw images, pixel format and image dimension information must be provided in a single DataLayout, either in the configuration record or in the data record of the image. Typically, the pixel format and image resolution will not change without a configuration change. Therefore, they are best stored in a configuration record. On the other hand, the keyframe timestamp and the keyframe index properties change with every frame, and are therefore searched only in a DataLayout that must be immediately before the image/video content block.  ","version":"Next","tagName":"h3"},{"title":"Reading Images​","type":1,"pageTitle":"Image Support","url":"/vrs/docs/ImageSupport#reading-images","content":" Image data is received by the RecordFormatStreamPlayer::onImageRead() callback:  bool onImageRead( const CurrentRecord&amp; record, size_t blockIndex, const ContentBlock&amp; contentBlock);   The RecordFormatStreamPlayer::onDataLayoutRead() callback happens after the DataLayout data has been read from disk.  The RecordFormatStreamPlayer::onImageRead() callback happens before any image data has been read. However, the provided contentBlock object holds all the image properties, so the following information can be found:  For all types: the contentBlock size is known. That's the size of the buffer you will need to read the image data stored in the image content block.For image/raw images: resolution, strides, and pixel format are defined.For image/custom_codec images: codec name is defined. Resolution, strides, and pixel format might be defined too, if provided in the stream using the Datalayout Conventions.For image/video images: the pixel format, resolution, codec name, keyframe timestamp, and keyframe index are defined.  The image data itself can be read using the RecordReader object provided by the CurrentRecord object. The image is described by an ImageContentBlockSpec object provided by contentBlock.image().  ","version":"Next","tagName":"h2"},{"title":"image/raw images​","type":1,"pageTitle":"Image Support","url":"/vrs/docs/ImageSupport#imageraw-images","content":" Allocate or reuse a buffer where you can read the image data. Be careful to properly handle the image’s stride and stride2. stride describes the stride of the first plane (the number of bytes separating the first byte of each successive lines), while stride2 describes the stride of all the following planes, if the images is stored in a multi-plane format.  ","version":"Next","tagName":"h3"},{"title":"image/png, image/jpg, and image/jxl images​","type":1,"pageTitle":"Image Support","url":"/vrs/docs/ImageSupport#imagepng-imagejpg-and-imagejxl-images","content":" You will need to read and decode the compressed image data using a standard implementation.  ","version":"Next","tagName":"h3"},{"title":"image/custom_codec images​","type":1,"pageTitle":"Image Support","url":"/vrs/docs/ImageSupport#imagecustom_codec-images","content":" You will need to read and decode the compressed image data using your own implementation of the custom codec, which implementation is presumably unknown to VRS.  ","version":"Next","tagName":"h3"},{"title":"image/video images​","type":1,"pageTitle":"Image Support","url":"/vrs/docs/ImageSupport#imagevideo-images","content":" Support for image/video images is provided by the vrs/xprs module, that depends on ffmpeg, and exposes the codecs built with it. ","version":"Next","tagName":"h3"},{"title":"The vrsplayer App","type":0,"sectionRef":"#","url":"/vrs/docs/vrsplayer","content":"","keywords":"","version":"Next"},{"title":"Playback Controls​","type":1,"pageTitle":"The vrsplayer App","url":"/vrs/docs/vrsplayer#playback-controls","content":" To play/pause/stop playback, use the following controls.    The Previous and Next Frame buttons will play at most one frame backward or forward for each visible stream. The Speed controls let you choose to play slower or faster. Obviously, if there is too much data to process for your system, frames will be dropped.  ","version":"Next","tagName":"h2"},{"title":"Overlay Selection​","type":1,"pageTitle":"The vrsplayer App","url":"/vrs/docs/vrsplayer#overlay-selection","content":" The overlay selector lets you choose what information to display over the frames. Choose &quot;Hide&quot; to show nothing, &quot;Tags&quot; to show streams tags, and &quot;Configuration&quot;, &quot;State&quot; or &quot;Data&quot; to show the metadata found in the last record of that type.    Use the Text Overlay menubar options to control the color of the text, its font size, and whether the text is drawn on a solid background or not.    The vrsplayer app is improved regularly, so it's important to be able to discover functionality directly from the app's user interface. The following sections show less obvious features and controls.  ","version":"Next","tagName":"h2"},{"title":"Tooltips​","type":1,"pageTitle":"The vrsplayer App","url":"/vrs/docs/vrsplayer#tooltips","content":" To know the duration of the image data, use the tooltip found over the time display.  Note that the start and end times show the time range in which image or audio data was found. Streams that don't contain image or audio data are ignored, and only data records from image and audio streams are considered. So if a recording contains a single image stream that has a configuration record at timestamp 0 rather than just before the first data record (as is too often the practice), while the first data record is at timestamp 15, the playback start time will be 15.    The tooltip shown over frames shows the stream's ID (&quot;214-1&quot;), the RecordableTypeId name (&quot;RGB Camera Class&quot;), its flavor (if any, here &quot;device/aria&quot;), and if a stream tag named &quot;device_type&quot; is found, the device type (&quot;Simulated&quot;).    ","version":"Next","tagName":"h2"},{"title":"Menu Bar Commands​","type":1,"pageTitle":"The vrsplayer App","url":"/vrs/docs/vrsplayer#menu-bar-commands","content":" The Menu Bar offers functionality that's easy to ignore, don't forget to look for more options there!  ","version":"Next","tagName":"h2"},{"title":"Layout​","type":1,"pageTitle":"The vrsplayer App","url":"/vrs/docs/vrsplayer#layout","content":"   The Layout menu lets you control the number rows and the number of streams shown per row. Layout Frames 4x2 means using 2 rows with up to 4 streams each. The layout configurations offered depend on the number of image streams visible.  ","version":"Next","tagName":"h3"},{"title":"Audio​","type":1,"pageTitle":"The vrsplayer App","url":"/vrs/docs/vrsplayer#audio","content":" If your file contains a VRS stream with audio data that contains more than one audio channel, the Audio menu will let you control which audio channel(s) to play. For instance, with an Aria file containing 7 audio channels, the audio menu might look like so:    In this situation, the Mono playback option was selected, so you can choose to play any of the 7 audio channels. If your system supports stereo playback (as most systems do), that channel will be played on both output channels. Use this option to listen to audio channels individually.  Assuming your system supports stereo playback, you can choose one of two stereo channel pairing modes: Auto Channel Pairing, and Manual Channel Pairing.    Auto Channel Pairing lets you select a first audio channel (&quot;left&quot;), and the next audio channel will automatically be used as the &quot;right&quot; channel. If that automatic selection is not what you need, use the Manual Channel Pairing option:    In this mode, you can select arbitrary channels for the &quot;left&quot; and &quot;right&quot; channel of your stereo playback.  ","version":"Next","tagName":"h3"},{"title":"Presets​","type":1,"pageTitle":"The vrsplayer App","url":"/vrs/docs/vrsplayer#presets","content":"   The Presets menu's top section lets you save and manage presets. Use the Save Preset command to save your favorite stream display and audio configurations, including stream orientation, stream order, and which streams are visible or hidden. To arrange the image streams, you can also use the options presented in the Context Menu section below.    Once at least one preset has been saved, you can recall or delete presets, which automatically get a keyboard shortcut for quick access.  ","version":"Next","tagName":"h3"},{"title":"Context Menu​","type":1,"pageTitle":"The vrsplayer App","url":"/vrs/docs/vrsplayer#context-menu","content":" Control the rotation and orientation of each stream using the context menu shown when right-clicking on each stream. The same menu lets you reorder streams, by bumping the chosen stream up or down one position in the order. You can also hide streams from this context menu. The last option lets you save the visible frame as png or jpg file.    To unhide streams, use one of the Layout menu commands that appear once at least one stream has been hidden.    ","version":"Next","tagName":"h2"},{"title":"Playback \"fps\" Display​","type":1,"pageTitle":"The vrsplayer App","url":"/vrs/docs/vrsplayer#playback-fps-display","content":" Normal playback tries to flow at timestamp speed, and frames will be dropped easily at different stages if needed to keep up. At the bottom of each view, you will find an individual fps counter, with 3 parts:    the first number (16) is the count of records read from disk per second. During playback, it should be the highest of all the counters. That step includes reading data from storage, lossless decompression, data interpretation and StreamPlayer callbacks processing. Reading files is a single threaded operation (as file APIs typically are), so this work is done by same thread for all streams, which is no decoding or conversion work is done on that thread.the second number (4) is the count of frames &quot;decoded&quot; per second. If images are encoded (jpg/png) they will be decoded. Then if the frames need to be transcoded to a different pixel format for display, they'll be converted to a Qt friendly pixel format. If processing is too expensive, frames might be skipped at that stage. That second number can only be equal or lower than the first. This processing is happening on a dedicated thread for each stream.the last number (2) is the &quot;display&quot; fps (&quot;frames per second&quot;), which is the number of times the code that draws the frame in the Qt widget is called each second. During playback, the number should be equal or lower than the second, but if you resize the windows, that number can go way up, as even if playback is paused and the two first are frozen, this counter will be updated as the window is updated. All draw operations happen in the app's single UI thread.  Putting it all together, for replay, one thread reads the files and extracts the raw image data, one thread draws the processed images in the user interface, but each stream has its own thread to do its image processing independently.  ","version":"Next","tagName":"h2"},{"title":"Keyboard Playback Controls​","type":1,"pageTitle":"The vrsplayer App","url":"/vrs/docs/vrsplayer#keyboard-playback-controls","content":" Playback can be directly controlled from the keyboard:  Use the space bar to play/pause replay.The backspace and the home keys will reset playback to the start of the file, much like the Stop button.The left and right arrow keys will read at most one frame per stream, in either direction.The up and down arrow keys will jump at most 10 frames, in either direction.The page-up and page-down keys will jump at most 100 frames, in either direction.  When using the arrow keys, all frames are guaranteed to be read. Use this feature if you want to be sure to view every frame of your file. ","version":"Next","tagName":"h2"},{"title":"The vrs Command Line Tool","type":0,"sectionRef":"#","url":"/vrs/docs/VrsCliTool","content":"","keywords":"","version":"Next"},{"title":"File Inspection​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#file-inspection","content":" To peek into a VRS file, simply run:  vrs vrs/oss/test_data/VRS_Files/sample_file.vrs   note This is the only command that doesn't expect a command name to be specified as the first parameter.  A sample output is:  VRS file: 'vrs/oss/test_data/VRS_Files/sample_file.vrs', 81.1 KB. Tag: capture_time_epoch = 1636165950 -- Sat Nov 6 03:32:30 2021 CET Tag: os_fingerprint = MacOS 20.6.0 Tag: purpose = sample_code Found 3 devices, 307 records, 301 data records from 1493865.291 to 1493866.378 (1.087s, 275.9rps). 102 Generic Audio Stream #1 - sample/audio [101-1] records. VRS Tag: RF:Data:1 = audio/pcm/int16le/channels=1/rate=44100 VRS Tag: VRS_Original_Recordable_Name = Generic Audio Stream VRS Tag: VRS_Recordable_Flavor = sample/audio 1 configuration record, at 1493865.286. 1 state record, at 1493865.286. 100 data records, from 1493865.298 to 1493866.378 (1.081s, 91.6rps). 103 Forward Camera Class #1 - sample/camera [200-1] records. VRS Tag: DL:Configuration:1:0 = {&quot;data_layout&quot;:[{&quot;name&quot;:&quot;image_width&quot;,&quot;type&quot;:&quot;DataPieceValue&lt;uint32_t&gt;&quot;,&quot;offset&quot;:0},{&quot;name&quot;:&quot;image_height&quot;,&quot;type&quot;:&quot;D... VRS Tag: DL:Data:1:0 = {&quot;data_layout&quot;:[{&quot;name&quot;:&quot;exposure_time&quot;,&quot;type&quot;:&quot;DataPieceValue&lt;uint64_t&gt;&quot;,&quot;offset&quot;:0},{&quot;name&quot;:&quot;exposure&quot;,&quot;type&quot;:&quot;DataPieceVal... VRS Tag: RF:Configuration:1 = data_layout VRS Tag: RF:Data:1 = data_layout/size=24+image/raw VRS Tag: VRS_Original_Recordable_Name = Forward Camera Class VRS Tag: VRS_Recordable_Flavor = sample/camera Tag: Position = Top 1 configuration record, at 1493865.286. 1 state record, at 1493865.286. 101 data records, from 1493865.291 to 1493866.373 (1.082s, 92.46rps). 102 Motion Data Class #1 - sample/motion [372-1] records. VRS Tag: DL:Configuration:1:0 = {&quot;data_layout&quot;:[{&quot;name&quot;:&quot;some_motion_stream_param&quot;,&quot;type&quot;:&quot;DataPieceValue&lt;double&gt;&quot;,&quot;offset&quot;:0}]} VRS Tag: DL:Data:1:0 = {&quot;data_layout&quot;:[{&quot;name&quot;:&quot;motion_data&quot;,&quot;type&quot;:&quot;DataPieceVector&lt;Matrix3Dd&gt;&quot;,&quot;index&quot;:0}]} VRS Tag: RF:Configuration:1 = data_layout/size=8 VRS Tag: RF:Data:1 = data_layout VRS Tag: VRS_Original_Recordable_Name = Motion Data Class VRS Tag: VRS_Recordable_Flavor = sample/motion 1 configuration record, at 1493865.286. 1 state record, at 1493865.286. 100 data records, from 1493865.297 to 1493866.378 (1.081s, 91.61rps).   That's a lot of information, that can be overwhelming the first time you see what's in a VRS file!  ","version":"Next","tagName":"h2"},{"title":"Part 1: Global File Information​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#part-1-global-file-information","content":" VRS file: 'vrs/oss/test_data/VRS_Files/sample_file.vrs', 81.1 KB. Tag: capture_time_epoch = 1636165950 -- Sat Nov 6 03:32:30 2021 CET Tag: os_fingerprint = MacOS 20.6.0 Tag: purpose = sample_code Found 3 devices, 307 records, 301 data records from 1493865.291 to 1493866.378 (1.087s, 275.9rps).   file chunks &amp; sizesfile tagscount of streams: &quot;found 3 devices&quot;count of records (any types): &quot;307 records&quot;count of data records: &quot;301 data records&quot;data records timestamp range (if applicable): &quot;data records from 1493865.291 to 1493866.378&quot;time duration when data records are available (if applicable): &quot;1.087s&quot;data records rate in rps (&quot;records per second&quot;, if applicable): &quot;275rps&quot;  ","version":"Next","tagName":"h3"},{"title":"Part 2: Stream Information (Once per Stream)​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#part-2-stream-information-once-per-stream","content":" 103 Forward Camera Class #1 - sample/camera [200-1] records. VRS Tag: DL:Configuration:1:0 = {&quot;data_layout&quot;:[{&quot;name&quot;:&quot;image_width&quot;,&quot;type&quot;:&quot;DataPieceValue&lt;uint32_t&gt;&quot;,&quot;offset&quot;:0},{&quot;name&quot;:&quot;image_height&quot;,&quot;type&quot;:&quot;D... VRS Tag: DL:Data:1:0 = {&quot;data_layout&quot;:[{&quot;name&quot;:&quot;exposure_time&quot;,&quot;type&quot;:&quot;DataPieceValue&lt;uint64_t&gt;&quot;,&quot;offset&quot;:0},{&quot;name&quot;:&quot;exposure&quot;,&quot;type&quot;:&quot;DataPieceVal... VRS Tag: RF:Configuration:1 = data_layout VRS Tag: RF:Data:1 = data_layout/size=24+image/raw VRS Tag: VRS_Original_Recordable_Name = Forward Camera Class VRS Tag: VRS_Recordable_Flavor = sample/camera Tag: Position = Top 1 configuration record, at 1493865.286. 1 state record, at 1493865.286. 101 data records, from 1493865.291 to 1493866.373 (1.082s, 92.46rps).   Count of records in that stream: &quot;103&quot; ... &quot;records.&quot;RecordableTypeID name: &quot;Forward Camera Class&quot;Stream instance ID: &quot;#1&quot;Stream flavor (if used): &quot;sample/camera&quot;Stream identifier: &quot;200-1&quot;. 200 is the stream's RecordableTypeId numeric value, and 1 is the stream's instance ID.Stream VRS tags, which show, among other things, the stream's RecordFormat and DataLayout definitions in json form. These definitions are truncated if they are too long. See the record-formats sub-command below if you need complete definitions.Stream (user) tags.count of Configuration records, timestamp range and rps when there are more than one.count of State records, timestamp range and rps when there are more than one.count of Data records, timestamp range and rps when there are more than one.  All that information is presented in a human readable format, but you can get the same information in json form using:  vrs json-description file.vrs   ","version":"Next","tagName":"h3"},{"title":"Data Exploration/Printing​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#data-explorationprinting","content":" ","version":"Next","tagName":"h2"},{"title":"Basic Overview​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#basic-overview","content":" Commands: list.  To list records with their top-level metadata, simply run:  vrs list file.vrs   This command lists all the records with timestamp, stream name and instance ID, stream identifier, the record type, and the size of the uncompressed record payload. &quot;Uncompressed&quot; refers to the file format lossless compression using lz4 or zstd that might be happening at the record level, rather than image or video codec compression. Records are treated as a black box, so this command will work with any VRS file. For instance:  1493865.286 Motion Data Class #1 [372-1], Configuration record, 8 bytes total. 1493865.286 Motion Data Class #1 [372-1], State record, 0 bytes total. 1493865.286 Generic Audio Stream #1 [101-1], Configuration record, 0 bytes total. 1493865.286 Generic Audio Stream #1 [101-1], State record, 0 bytes total. 1493865.286 Forward Camera Class #1 [200-1], Configuration record, 44 bytes total. 1493865.286 Forward Camera Class #1 [200-1], State record, 0 bytes total. 1493865.291 Forward Camera Class #1 [200-1], Data record, 307224 bytes total. ...   ","version":"Next","tagName":"h3"},{"title":"Taking Advantage of RecordFormat and DataLayout​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#taking-advantage-of-recordformat-and-datalayout","content":" Commands: print, print-details, print-json, print-json-pretty.  When records are represented using RecordFormat and DataLayout, it is possible to interpret a record's payload. VRS can recognize the record's successive content blocks, their type, and print metadata blocks otherwise known as DataLayout blocks in a human readable form, or in json.  vrs print file.vrs   This command lists all the records with timestamp, stream name and instance ID, stream identifier, the record type, the record's RecordFormat definition, and the size of the uncompressed record payload. This format is meant to be most human readable, even if it requires a little getting used to. Notice that the text indentation to help group information together.  For instance:  1493865.286 Motion Data Class #1 [372-1], Configuration record, data_layout/size=8 = 8 bytes total. - DataLayout: some_motion_stream_param: 25 1493865.286 Motion Data Class #1 [372-1], State record, &lt;no RecordFormat definition&gt; = 0 bytes total. 1493865.286 Generic Audio Stream #1 [101-1], Configuration record, &lt;no RecordFormat definition&gt; = 0 bytes total. 1493865.286 Generic Audio Stream #1 [101-1], State record, &lt;no RecordFormat definition&gt; = 0 bytes total. 1493865.286 Forward Camera Class #1 [200-1], Configuration record, data_layout = 44 bytes total. - DataLayout: image_width: 640 image_height: 480 image_pixel_format: grey8 (1) camera_calibration: 23 53 343 3 12 8 1493865.286 Forward Camera Class #1 [200-1], State record, &lt;no RecordFormat definition&gt; = 0 bytes total. 1493865.291 Forward Camera Class #1 [200-1], Data record, data_layout/size=24+image/raw = 307224 bytes total. - DataLayout: exposure_time: 47390873 exposure: 2.5 frame_counter: 0 camera_temperature: 38.5 - Image block, image/raw/640x480/pixel=grey8, 307200 bytes. 1493865.297 Motion Data Class #1 [372-1], Data record, data_layout = 8 bytes total. - DataLayout: motion_data: 1493865.298 Generic Audio Stream #1 [101-1], Data record, audio/pcm/int16le/channels=1/rate=44100 = 512 bytes total. - Audio block, audio/pcm/int16le/channels=1/rate=44100/samples=256, 512 bytes. ...   In the output above, data_layout/size=8 and data_layout/size=24+image/raw are RecordFormat definitions. When records have such a definition, then their content is printed out, depending on the content block types: DataLayout content blocks, aka, metadata blocks, are printed field by field, with name and values. An overview of image blocks and audio blocks are printed.  A variation of this command provides more low-level details than most users need:  vrs print-details file.vrs   The output is similar, but notice how the metadata blocks contain more information, field types, field block offsets and field sizes:  1493865.286 Motion Data Class #1 [372-1], Configuration record, data_layout/size=8 = 8 bytes total. - DataLayout: 1 fixed size pieces, total 8 bytes. some_motion_stream_param (double) @ 0+8 Value: 25 1493865.286 Motion Data Class #1 [372-1], State record, &lt;no RecordFormat definition&gt; = 0 bytes total. 1493865.286 Generic Audio Stream #1 [101-1], Configuration record, &lt;no RecordFormat definition&gt; = 0 bytes total. 1493865.286 Generic Audio Stream #1 [101-1], State record, &lt;no RecordFormat definition&gt; = 0 bytes total. 1493865.286 Forward Camera Class #1 [200-1], Configuration record, data_layout = 44 bytes total. - DataLayout: 3 fixed size pieces, total 20 bytes. image_width (uint32_t) @ 0+4 Value: 640 image_height (uint32_t) @ 4+4 Value: 480 image_pixel_format (uint32_t) @ 8+4 Value: grey8 (1) 1 variable size pieces, total 24 bytes. camera_calibration (vector&lt;float&gt;) @ index: 0, count: 6 Values: 23 53 343 3 12 8 1493865.286 Forward Camera Class #1 [200-1], State record, &lt;no RecordFormat definition&gt; = 0 bytes total. 1493865.291 Forward Camera Class #1 [200-1], Data record, data_layout/size=24+image/raw = 307224 bytes total. - DataLayout: 4 fixed size pieces, total 24 bytes. exposure_time (uint64_t) @ 0+8 Value: 47390873 exposure (float) @ 8+4 Value: 2.5 frame_counter (uint64_t) @ 12+8 Value: 0 camera_temperature (float) @ 20+4 Value: 38.5 - Image block, image/raw/640x480/pixel=grey8, 307200 bytes. 1493865.297 Motion Data Class #1 [372-1], Data record, data_layout = 8 bytes total. - DataLayout: 1 variable size pieces, total 0 bytes. motion_data (vector&lt;Matrix3Dd&gt;) @ index: 0, count: 0 1493865.298 Generic Audio Stream #1 [101-1], Data record, audio/pcm/int16le/channels=1/rate=44100 = 512 bytes total. - Audio block, audio/pcm/int16le/channels=1/rate=44100/samples=256, 512 bytes. ...   Interpretation of image_width (uint32_t) @ 0+4 Value: 640:  image_width is the field's name,uint32_t is the field's type,@ 0+4 means the data starts at byte offset 0 and uses 4 bytes (the size of uint32_t),640 is the field's actual value.  If you plan on parsing the output with code, then json is probably a better format. You can use:  vrs print-json file.vrs   Output:  {&quot;record&quot;:{&quot;timestamp&quot;:1493865.286,&quot;device&quot;:&quot;372-1&quot;,&quot;type&quot;:&quot;Configuration&quot;,&quot;size&quot;:8}} {&quot;data_layout&quot;:[{&quot;value&quot;:25.0,&quot;name&quot;:&quot;some_motion_stream_param&quot;,&quot;type&quot;:&quot;Value&lt;double&gt;&quot;}]} {&quot;record&quot;:{&quot;timestamp&quot;:1493865.286,&quot;device&quot;:&quot;372-1&quot;,&quot;type&quot;:&quot;State&quot;,&quot;size&quot;:0}} {&quot;record&quot;:{&quot;timestamp&quot;:1493865.286,&quot;device&quot;:&quot;101-1&quot;,&quot;type&quot;:&quot;Configuration&quot;,&quot;size&quot;:0}} {&quot;record&quot;:{&quot;timestamp&quot;:1493865.286,&quot;device&quot;:&quot;101-1&quot;,&quot;type&quot;:&quot;State&quot;,&quot;size&quot;:0}} {&quot;record&quot;:{&quot;timestamp&quot;:1493865.286,&quot;device&quot;:&quot;200-1&quot;,&quot;type&quot;:&quot;Configuration&quot;,&quot;size&quot;:44}} {&quot;data_layout&quot;:[{&quot;value&quot;:640,&quot;name&quot;:&quot;image_width&quot;,&quot;type&quot;:&quot;Value&lt;uint32_t&gt;&quot;},{&quot;value&quot;:480,&quot;name&quot;:&quot;image_height&quot;,&quot;type&quot;:&quot;Value&lt;uint32_t&gt;&quot;},{&quot;value&quot;:1,&quot;name&quot;:&quot;image_pixel_format&quot;,&quot;type&quot;:&quot;Value&lt;uint32_t&gt;&quot;},{&quot;value&quot;:[23.0,53.0,343.0,3.0,12.0,8.0],&quot;name&quot;:&quot;camera_calibration&quot;,&quot;type&quot;:&quot;Vector&lt;float&gt;&quot;}]} {&quot;record&quot;:{&quot;timestamp&quot;:1493865.286,&quot;device&quot;:&quot;200-1&quot;,&quot;type&quot;:&quot;State&quot;,&quot;size&quot;:0}} {&quot;record&quot;:{&quot;timestamp&quot;:1493865.291,&quot;device&quot;:&quot;200-1&quot;,&quot;type&quot;:&quot;Data&quot;,&quot;size&quot;:307224}} {&quot;data_layout&quot;:[{&quot;value&quot;:47390873,&quot;name&quot;:&quot;exposure_time&quot;,&quot;type&quot;:&quot;Value&lt;uint64_t&gt;&quot;},{&quot;value&quot;:2.5,&quot;name&quot;:&quot;exposure&quot;,&quot;type&quot;:&quot;Value&lt;float&gt;&quot;},{&quot;value&quot;:0,&quot;name&quot;:&quot;frame_counter&quot;,&quot;type&quot;:&quot;Value&lt;uint64_t&gt;&quot;},{&quot;value&quot;:38.5,&quot;name&quot;:&quot;camera_temperature&quot;,&quot;type&quot;:&quot;Value&lt;float&gt;&quot;}]} {&quot;image&quot;:&quot;image/raw/640x480/pixel=grey8&quot;} {&quot;record&quot;:{&quot;timestamp&quot;:1493865.297,&quot;device&quot;:&quot;372-1&quot;,&quot;type&quot;:&quot;Data&quot;,&quot;size&quot;:8}} {&quot;data_layout&quot;:[{&quot;name&quot;:&quot;motion_data&quot;,&quot;type&quot;:&quot;Vector&lt;Matrix3Dd&gt;&quot;}]} {&quot;record&quot;:{&quot;timestamp&quot;:1493865.298,&quot;device&quot;:&quot;101-1&quot;,&quot;type&quot;:&quot;Data&quot;,&quot;size&quot;:512}} {&quot;audio&quot;:&quot;audio/pcm/int16le/channels=1/rate=44100/samples=256&quot;} ...   Notice that every record starts with {&quot;record&quot;:{&quot;timestamp&quot;:..., follow by a number of content blocks lines. Because DataLayout blocks are printed in a single line json blob, they can be difficult for a human to read. Use vrs print-json-pretty file.vrs to get the same content, but with DataLayout blocks printed out using indented json.  {&quot;record&quot;:{&quot;timestamp&quot;:1493865.286,&quot;device&quot;:&quot;372-1&quot;,&quot;type&quot;:&quot;Configuration&quot;,&quot;size&quot;:8}} { &quot;data_layout&quot;: [ { &quot;value&quot;: 25.0, &quot;name&quot;: &quot;some_motion_stream_param&quot;, &quot;type&quot;: &quot;Value&lt;double&gt;&quot; } ] } {&quot;record&quot;:{&quot;timestamp&quot;:1493865.286,&quot;device&quot;:&quot;372-1&quot;,&quot;type&quot;:&quot;State&quot;,&quot;size&quot;:0}} ...   All the commands above rely on the RecordFormat and DataLayout definitions embedded in your file. Each stream has a copy of the definitions needed to interpret its own records. To print these definitions, use:  vrs record-formats file.vrs   ...which will show something like:  ... 1100-7 Polaris Camera #7 Data v1: data_layout/size=24+image/raw Content block 0: { &quot;data_layout&quot;: [ { &quot;name&quot;: &quot;hal_frame_counter&quot;, &quot;type&quot;: &quot;Value&lt;uint64_t&gt;&quot; }, { &quot;name&quot;: &quot;hal_exposure_time&quot;, &quot;type&quot;: &quot;Value&lt;double&gt;&quot; }, { &quot;name&quot;: &quot;hal_arrival_time&quot;, &quot;type&quot;: &quot;Value&lt;double&gt;&quot; } ] } ...   Which means: For the stream 1100-7 named &quot;Polaris Camera&quot;, stream instance ID 1, the record format definition for data records version 1 is data_layout/size=24+image/raw. A plus sign '+' separates successive content blocks, so here, we have two content blocks. Content block index 0 is data_layout/size=24, and content block index 1 is image/raw.  The line starting with Content block 0 provides the DataLayout definition of the content block index 0, which has 3 values, an uint64_t field, followed by two double fields, for a total of 24 bytes. So no matter what the record's content is, the first content block will always use 24 bytes (before lossless compression), which is why that content block's definition is data_layout/size=24.  It might be useful to examine RecordFormat definitions one stream at a time. This can be done easily using stream filtering (fully documented below):  vrs record-formats file.vrs + 1100-7   ","version":"Next","tagName":"h2"},{"title":"Record Filtering​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#record-filtering","content":" Because printing everything is rarely what you need (VRS files usually contain way too much data!), you are likely to want to restrict the range of records to print or &quot;consider&quot; for your operation, be it data printing, data extraction, copy, etc.  Specifying record filters options as additional parameters works with most commands for which it makes sense.  Filtering by Timestamp​  Options: --before [+|-]&lt;max-timestamp&gt;, --after [+|-]&lt;min-timestamp&gt;, --range [+|-]&lt;min-timestamp&gt; [+|-]&lt;max-timestamp&gt;, --around [+|-]&lt;timestamp&gt; &lt;time-range&gt;.  To only print records which timestamps is less than the absolute timestamp 1493866, do:  vrs print file.vrs --before 1493866   Because timestamps vary from file to file, there are shortcuts to compute timestamps relative to the file's first and last data records. By convention, timestamps specified with a '+' sign are relative to the first data record's timestamp. Timestamps specified with a '-' sign are relative to the last data record's timestamp.  Therefore, to print records up to 1 second after the first data record, do:  vrs print file.vrs --before +1   To skip the first second and the last second of data records, use:  vrs print file.vrs --after +1 --before -1   ...or:  vrs print file.vrs --range +1 -1   To focus around a particular timestamp, for instance, to print 1 second of records around a particular timestamp:  vrs print file.vrs --around 1493866 1   Filtering by Record Type and by Stream​  Options: + [configuration|state|data], - [configuration|state|data].  Options: + &lt;recordable_type_id&gt;, - &lt;recordable_type_id&gt;, + &lt;recordable_type_id&gt;-&lt;instance_id&gt;, - &lt;recordable_type_id&gt;-&lt;instance_id&gt;.  important Notice the space between the + or - sign, and the argument that follows!  + options allow you to include a particular record type, all the streams with a particular recordable type ID, or a stream fully identified by recordable type ID and instance ID. - options allow you to exclude a particular record type, all the streams with a particular recordable type ID, or the stream fully identified by recordable type ID and instance ID. If the first of the options starts with a +, it is assumed nothing was included before for that category (record type or streams), whereas if the first option is -, it is assumed that all the elements for that category is the starting set. In other words + configuration will filter out all the record types except configuration records, whereas - configuration will filter out all configuration records.  These options function in accumulation, left to right, so you can include all the stream with the recordable type ID 1100 except the one with the instance ID 3 by doing: + 1100 - 1100-3.  First Records Filter​  Option: --first-records.  For debugging purposes, it can be very convenient to only print the information relating to the first record of each type of each stream. For instance, to get an idea of what every stream in a file looks like:  vrs print file.vrs --first-records   Of course, you can combine this option with stream filtering:  vrs print file.vrs --first-records + 1100-7   Decimation (Advanced)​  Options: --decimate &lt;recordable_type_id&gt;[-&lt;instance_id&gt;] &lt;timestamp_interval&gt;, --bucket-interval &lt;timestamp_interval&gt;, --bucket-max-delta &lt;timestamp_delta&gt;.  The decimation options let your drop data records based on their timestamp relative to other records. For instance, you might only want at most one data record per second for a particular stream, dropping many records, as necessary.  For instance, compare:  vrs list vrs/oss/test_data/VRS_Files/simulated.vrs + 214-1  to:  vrs list vrs/oss/test_data/VRS_Files/simulated.vrs + 214-1 --decimate 214-1 1.  The options --bucket-interval and --bucket-max-delta work together, and work across all streams. Use --bucket-interval to create &quot;buckets&quot; at regular intervals throughout the file, from which at most one record per stream will be kept. Add the --bucket-max-delta to restrict the size of these buckets (this option doesn't work alone).  ","version":"Next","tagName":"h3"},{"title":"Data Extraction​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#data-extraction","content":" Commands: extract-images, extract-audio, extract-all.  These commands let you extract images, audio, and metadata into files. Use the --to &lt;folder_path&gt; to specify a destination folder where the data is to be extracted, or it will be added to the current working directory.  For instance, to extract images from a VRS file into image files, use:  vrs extract-images file.vrs --to image-folder   Note that images in RAW format will be saved as png files, and that this operation might require a lossy pixel format conversion. To avoid that, use the --raw-images option, which will make RAW images be saved in .raw files, with a name that will provide the actual image specifications. Such a name might be 214-1-00099-147.293-bayer8_rggb-1408x1408-stride_1408.raw, where:  214-1 is the stream identifier,00099 the image number in that stream,147.293 the image's timestamp,bayer8_rggb is the pixel format,1408x1408 are the image dimensions,stride_1408 tells that there are 1408 bytes of data per line.  extract-audio will extract audio data in .wav files, one per stream.  extract-all will extract images, audio data, and metadata. The metadata is extracted in a single .jsons file that contains a succession of json messages, one per line. Each line corresponds to a single record, in timestamp order, so it is possible to parse it even if the number of records is huge. Saving all the data in a single file prevents saturating your disk with possibly millions of small files.  Of course, the filtering options work with the extract commands, so you can extract only some of the streams, or only a few seconds of recording, etc. For instance, the following command will extract images from the 200-1 stream only, that are in the 10 first seconds of that stream, and at most 1 image per second of data.  vrs extract-images file.vrs --to image-folder + 200-1 --before +10 --decimate 200-1 1   ","version":"Next","tagName":"h2"},{"title":"Copy Operation​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#copy-operation","content":" Commands: copy, merge.  ","version":"Next","tagName":"h2"},{"title":"Copying Files​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#copying-files","content":" The primary use of the copy command is to copy records from an existing VRS into a new VRS file. Use the filter options in conjunction with the copy command to selectively copy parts of a file into a new one. With the filters you can do things like select a subset of streams, truncate a file by timestamp, or decimate a file.  In its most basic form, a copy operation looks like this:  vrs copy original.vrs --to new.vrs   This will simply copy all the records from original.vrs and create a new VRS file named new.vrs that contains the same data. It is important to note that such a copy operation:  processes all the records one after the other.at the lowest level, all the records are decompressed, then recompressed in the new file, maybe using a different lossless compression setting. Commonly, during capture, files are compressed only very lightly, because data is generated too quickly and/or enough CPU bandwidth isn't available to compress more. During a copy operation, by default, zstd is used at the &quot;zlight&quot; level (zstd level &quot;3&quot;), but you can select a different lossless compression setting, such as &quot;zmedium&quot; (zstd level 5), or &quot;ztight&quot; (zstd level 18). Note that if decompression speed is critical for you, you can use lz4 instead of zstd, at the expense of compression ratio.the file is &quot;cleaned-up&quot;, records are sorted if they were out of order, and an optimized index is created, possibly making the file slightly more efficient to read.  To control the lossless compression applied at the record level, use the --compression=&lt;level&gt; option, where &lt;level&gt; is one of none, fast, tight, zfast, zlight, zmedium, ztight, zmax. The compression levels which name starts with a z use zstd, while the others use lz4. In practice, zlight or zmedium are likely to be your best options, because ztight and zmax usually offer marginal size gains while being too slow to be practical.  To copy 10 seconds of a file with zmedium compression while skipping the first 2 seconds of data records, use:  vrs copy original.vrs --to new.vrs --range +2 +12 --compression=zmedium   important Stream instance ID numbers might not be preserved in the output VRS file. Stream instance IDs are not guaranteed during file creation, and they are not necessarily preserved during copy operations.  ","version":"Next","tagName":"h3"},{"title":"Copying Files While Modifying Data​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#copying-files-while-modifying-data","content":" Overwriting Tags​  VRS file content modifications during copies are possible, but only simple modifications are offered by the vrs command line tool.  Overwriting file and stream tags can be very useful.  --file-tag &lt;tag_name&gt; &lt;tag_value&gt; will let you set or overwrite a file tag, while --stream-tag &lt;stream-id&gt; &lt;tag_name&gt; &lt;tag_value&gt; will let you set or overwrite a stream tag. For instance:  vrs copy original.vrs --to new.vrs --file-tag &quot;date&quot; &quot;April 1, 2022&quot; vrs copy original.vrs --to new.vrs --stream-tag 1100-1 &quot;position&quot; &quot;top_left&quot;   You can set or overwrite as many file and stream tags as you need, by using the --file-tag and --stream-tag options as many times as you need in the same command.  Zero Files​  The --zero option lets you copy all the data. Clearing all image and audio data with zeros will dramatically reduce the size of files that have this type of data. This is mainly due to the lossless compression at the record level. Zeroed files can be over 99% smaller than the original files. This can be useful for several reasons:  you now have a lightweight version of a file, that contains all its metadata.since images and audio data are cleared, the copy might be anonymized, should there be no sensitive data in the metadata of the file.this file is identical to the original, except for the image and audio data (obviously), which can make it convenient to share over the internet to debug issues not related to the image and audio data.  ","version":"Next","tagName":"h3"},{"title":"Merging Files​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#merging-files","content":" The copy and merge command let you merge files together, simply by specifying more than one input files:  vrs copy original.vrs original2.vrs original3.vrs --to new.vrs vrs merge original.vrs original2.vrs original3.vrs --to new.vrs   danger Attention, while both copy and merge allow you to merge multiple files into a single output VRS file, their behavior is very different!  Because VRS files must have timestamps in a single time domain, it is assumed that all the files merged are using the same time domain, but effectively, no validation is performed for you: you need to know what you're doing!  File tags are merged. In case of name collision and the values are different, the first instance &quot;wins&quot;. The other values of the tag are preserved, by adding a tag with a different name: &quot;name&quot; becomes &quot;name_merged&quot;, then &quot;name_merged-1&quot; if there are more conflicting values, etc.  Merging Files with copy​  When handling multiple source files, the copy operation copies all the streams side-by-side in the output file, so that the number of streams in the output file will always be the sum of the number of streams in all the input files, even if two or more streams in input files have the same stream identifier. When streams with the same recordable type ID are copied in a single file, the stream instance IDs will be ordered so that the streams with a particular recordable type ID from the first file will have the lowest instance IDs, in the same order they had in their source file, then come the streams from the next file, etc.  Example:  File 1: 100-1, 100-2, 200-2 File 2: 100-1 File 3: 300-1, 200-1 Output: 100-1, 100-2, 100-3, 200-1, 200-2, 300-1 Source: File1 File1 File2 File1 File3 File3   Notice how the &quot;200&quot; stream are ordered based on their original file, not their original instance ID.  Merging Files with merge​  The merge operation merges streams with the same recordable type ID, one per file, into one stream, in sequence in the output file. Records with the exact same timestamp are deduplicated, by keeping only the record from the earliest file. This behavior is usually only needed when files have been truncated by timestamp and they are merged together again.  Example:  File 1: 100-1, 100-2, 200-2 File 2: 100-1 File 3: 300-1, 200-1 Output: 100-1, 100-2, 200-1, 300-1 Source: File1 File1 File2 File3 + + File2 File3   Notice how the &quot;200&quot; stream are now ordered based on their original file, not their original instance ID.  ","version":"Next","tagName":"h3"},{"title":"File Validation​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#file-validation","content":" Commands: check, checksum, checksums, checksum-verbatim, hexdump, compare, compare-verbatim.  ","version":"Next","tagName":"h2"},{"title":"File Integrity Validation​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#file-integrity-validation","content":" The check command &quot;simply&quot; decodes every record in the VRS file and prints how many records were decoded successfully. It proves that the VRS file is correct at the VRS level.  the file is actually a VRS file (!)the file has an index or it was rebuilt (see the logs to tell)the records are found where they were expectedthe records were read successfullythe records were decompressed successfully (VRS-internal lossless decompression only).  vrs check file.vrs   Records are all referenced in the file's index. They also have a header in the file just before the actual record payload. These two pieces of metadata must match otherwise an error is reported. Records are usually losslessly compressed with lz4 or zstd when record size exceeds a certain number (around 200 bytes). This provides some level of corruption detection, because compressed payloads have an internal checksum.  This type of file corruption rarely happens, and can usually be traced back to a corrupt file transfer.  ","version":"Next","tagName":"h3"},{"title":"Checksums​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#checksums","content":" A logical data checksum can be generated for a VRS file, which will checksum:  the file tagsthe sequence of streams for each recordable type ID, but not their actual instance IDthe stream tags, both VRS tags and user tagsthe metadata of all the recordsthe payload of all the records.  vrs checksum file.vrs   The intent is that you can copy a VRS file using the copy command, change the lossless compression level, and though the files are very different at the file system level, they will still be identical from a VRS logical content and checksum standpoint.  If two files do not have the same checksum, use the checksums command to have separate checksums, which should reveal which parts are different.  If you can checksum any file at the file system level, regardless of whether the file is a VRS file or not, using the checksum-verbatim command.  ","version":"Next","tagName":"h3"},{"title":"Viewing Records​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#viewing-records","content":" If you are curious about how records look like inside a VRS file, you can use the hexdump command. This will print record payloads, uncompressed, in hexadecimal, after a minimal header per record. You will probably want to combine this option with the filtering options, to limit the size of the output.  ","version":"Next","tagName":"h3"},{"title":"Comparing Files​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#comparing-files","content":" You can compare files at the logical level, with the compare command.  vrs compare file1.vrs file2.vrs   The output will tell you whether the files are identical, and if they are not, which part(s) aren't.  You can also use the compare-verbatim command to compare files at the file system byte level.  vrs compare-verbatim file1.vrs file2.vrs   ","version":"Next","tagName":"h3"},{"title":"Advanced Options​","type":1,"pageTitle":"The vrs Command Line Tool","url":"/vrs/docs/VrsCliTool#advanced-options","content":" Commands: fix-index, debug, rage.  If a VRS recording was interrupted during creation, the VRS file probably won't have a proper index, because it is written to disk last at the end of the file. Maybe the recording app crashed, or the system ran out of disk space. By design, this situation is ok, and whatever was written is &quot;safe&quot;: an index will be rebuilt by scanning the file, discovering records and their metadata by walking the list of daisy-chained records. But this process can be slow if the file is large and/or the disk slow. In order to &quot;fix&quot; such files, you have two options:  copy the recording using the vrs copy command. The new recording will have a proper index.use the fix-index command, to patch the original file in place. The command might truncate the end of the file to get rid of incomplete data, which is usually simply the last record what wasn't fully written out, then the rebuilt index will be written out at the end of the file, provided there is enough disk space for it. Because the original recording file is modified, it is a bit riskier, but much faster, than copying the file. Note that normally, the resulting file contains the same data, as partially written records can't be recovered by VRS either way.  vrs fix-index file1.vrs   To produce enough information for a bug report about a file that's not working right, you use the command:  vrs rage file.vrs   This command simply produces the combined information of vrs details and vrs print-details file.vrs --first-records in one easy to remember command.  To check some VRS internal data structures and display some diagnostic information about a VRS file, use:  vrs debug file.vrs   This option is mostly meant for the VRS file format maintainers, and we mention it here for completeness. ","version":"Next","tagName":"h2"},{"title":"Record Format","type":0,"sectionRef":"#","url":"/vrs/docs/RecordFormat","content":"","keywords":"","version":"Next"},{"title":"Record Format Version​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#record-format-version","content":" Each record in a stream has its own format version number, which is a uint32_t value. However, because records belong to a single stream and each has a record type (Configuration, State, or Data), format version numbers are only meaningful within that stream and for that record type. You do not need to worry about format version collisions between streams and record types.  Before RecordFormat was available, record format versioning was critical, because it was the only information about how the record's data was formatted. In the StreamPlayer callbacks you received when reading a file, you were responsible for interpreting every byte of data and you also had to manually manage all data format changes. Since record data formats were not self-described within the file, each time you needed to add, remove, or change a field, you had to change the format version, and handle a growing number of format versions explicitly in your code. This was unmanageable. Moreover, it was not possible to write standard tools that could interpret that was stored in records, show the images they might contains, or any other metadata.  RecordFormat and DataLayout were designed to solve these challenges, and since, record format version changes are very rarely needed. RecordFormat structures records as a succession of typed content blocks, embedding descriptions, including DataLayout definitions, in the stream itself. VRS uses these embedded descriptions to interpret records, calculate content block boundaries using DataLayout Conventions, and send parsed content blocks to RecordFormatStreamPlayer callbacks when reading a VRS file. With RecordFormat and the DataLayout Conventions, it is now possible to write generic tools like vrsplayer, that can let you explore what's in a VRS file without any prior knowledge of the use case in which the file was recorded.  ","version":"Next","tagName":"h2"},{"title":"RecordFormat​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#recordformat","content":" Use RecordFormat to describe records as a sequence of typed content blocks. This structure applies to configuration, state, and data records alike.  ","version":"Next","tagName":"h2"},{"title":"ContentBlock​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#contentblock","content":" The content block types are: image, audio, datalayout, and custom. VRS saves RecordFormat definitions as a string that is generated and parsed for you, but which was designed to be very expressive and compact. Content block descriptions may contain additional information, specific to the content type. Here are some examples of single content block RecordFormat definitions:  imageimage/pngimage/jpgimage/jxlimage/rawimage/raw/640x480/pixel=grb8image/raw/640x480/pixel=grey8/stride=648image/custom_codec/codec=my_experimentimage/videoimage/video/codec=H.264audioaudio/pcmaudio/pcm/uint24be/rate=32000/channels=1datalayoutdatalayout/size=48customcustom/format=my_own_payload_formatcustom/size=160  image and audio content blocks are pretty much what you expect when you read their text description. datalayout blocks contain structured metadata information. custom content blocks are blocks of raw data, which format is known only to you, and which you are responsible for interpreting.  You can assemble as many content blocks as you like in a record, which might look like this:  datalayout+image/rawdatalayout+datalayout+audio/pcm  Again, these text descriptions are generated and parsed for you, so you don't need to worry about their syntax.  The RecordFormat and DataLayout descriptions of a stream's records are stored in the VRS tags of the stream. You will only see these text descriptions when you are using tools to dump a stream's VRS tags. VRS tags are associated with each stream for VRS internal usage, and are kept separate from the user stream tags.  In practice, the majority of the records used in VRS today use one of the following record formats:  datalayout: for records containing a single metadata content block, which is typical of configuration records.datalayout+image/raw: for records containing some image specific metadata and the raw pixel data of an image.datalayout+image/jpg and datalayout+image/video: for records containing some image specific metadata and compressed image data.  ","version":"Next","tagName":"h3"},{"title":"Datalayout Content Blocks​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#datalayout-content-blocks","content":" Datalayout content blocks, commonly referred to as datalayouts, are DataLayout objects that hold a collection of DataPieceXXX objects, which are containers of POD values and strings. If you have never seen a DataLayout definition, look at the MyDataLayout definition in the DataLayout Examples section below.  DataLayout are typically struct objects containing a series of DataPieceXXX member variables, that each have their own type and text label. The supported DataPieceXXX types are:  DataPieceValue, a single value of POD type T:  Type: template &lt;class T&gt; DataPieceValue&lt;T&gt;;Example: DataPieceValue&lt;int32_t&gt; exposure{&quot;exposure&quot;};  DataPieceEnum, a single value of enum ENUM_TYPE with the underlying type POD_TYPE:  Type: template &lt;typename ENUM_TYPE, typename POD_TYPE&gt; DataPieceEnum&lt;ENUM_TYPE, POD_TYPE&gt;Example: DataPieceEnum&lt;PixelFormat, uint32_t&gt; pixelFormat{&quot;pixel_format&quot;};  DataPieceArray, a fixed size array of values of POD type T:  Type: template &lt;class T&gt; DataPieceArray&lt;T&gt;;Example: DataPieceArray&lt;float&gt; calibration{&quot;calibration&quot;, 25};  DataPieceVector, a vector of values of type T, which size may change for each record:  Type: template &lt;class T&gt; DataPieceVector&lt;T&gt;Example: DataPieceVector&lt;int8_t&gt; udpPayload{&quot;udp_payload&quot;};  DataPieceStringMap, the equivalent of std::map&lt;std::string, T&gt;:  Type: template &lt;class T&gt; DataPieceStringMap&lt;T&gt;Example: DataPieceStringMap&lt;Point2Di&gt; labelledPoints{&quot;labelled_points&quot;};  DataPieceString, a std::string value:  Type: DataPieceStringExample: DataPieceString message{&quot;message&quot;};  Template class T can be any of these built-in POD types:  Boolean (use vrs::Bool)Signed or unsigned integer (8, 16, 32, or 64 bits)32 bit float64 bit double  Template class T can also be any of these vector types (using float, double or int32_t for coordinates):  2, 3, or 4D points3 or 4D matrices  std::string can be used with DataPieceVector&lt;T&gt; and DataPieceStringMap&lt;T&gt;, but cannot be used with the other template types.  note Always use &lt;cstdint&gt; definitions. Never use native platform dependent types like short, int, long, or size_t, because their actual size will vary depending on the architecture or the compiler configuration.  ","version":"Next","tagName":"h3"},{"title":"DataLayout Format Resilience​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#datalayout-format-resilience","content":" DataLayout objects are structs, so it is very simple to add, remove, and reorder DataPieceXXX fields. But datalayouts definitions are very resilient to definition changes, so that even when making such changes, newer code can read older files, and older code can read newer files.  Datalayouts format resilience is possible, because each DataPieceXXX object is identified by a unique combination of these elements:  DataPiece typeLabelTemplate class T, except for DataPieceString  This unique combination is critical to providing datalayout forward/backward compatibility, without worrying about the actual placement of the data. If you change the type or the label of a field, you will change its signature, and it won't be recognized in older files. The modified field will look like a new field, and the data from older files will no longer be accessible using the updated definition.  DataLayout definitions do not support other types of containers or nested containers, because that would make it impossible to guarantee forward/backward compatibility. However, it is possible to use repeated and nested structs, using DataLayoutStruct, as shown in the second example below.  In some situations, such as when you need to save space, it is desirable to store some fields only in some conditions. The OptionalDataPieces template makes it easy to specify and control if a group of fields should be saved or not, but the choice must be made once for the whole file.  If you need more freedom, you can use a free form container such as JSON in a DataPieceString field. If you have binary data, you can use a DataPieceVector&lt;uint8_t&gt; field.  We recommend that you use lowercase snake_case as your naming convention for labels. This will limit problems if these names are used as keys in a Python dictionary, in particular when using pyvrs to create or read datalayouts.  ","version":"Next","tagName":"h3"},{"title":"DataLayout Examples​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#datalayout-examples","content":" Example 1: standard caseExample 2: nested definitionsExample 3: optional definitions Here is a sample DataLayout definition: struct MyDataLayout : public AutoDataLayout { // Fixed size pieces: std::string is NOT supported as a template type. DataPieceValue&lt;double&gt; exposureTime{&quot;exposure_time&quot;}; DataPieceValue&lt;uint64_t&gt; frameCounter{&quot;frame_counter&quot;}; DataPieceValue&lt;float&gt; cameraTemperature{&quot;camera_temperature&quot;}; DataPieceEnum&lt;PixelFormat, uint32_t&gt; pixelFormat{&quot;pixel_format&quot;}; DataPieceArray&lt;Matrix3Dd&gt; arrayOfMatrix3Dd{&quot;matrices&quot;, 3}; // array size = 3 // Variable size pieces: std::string is supported as a template type. DataPieceVector&lt;Point3Df&gt; vectorOfPoint3Df{&quot;points&quot;}; DataPieceVector&lt;string&gt; vectorOfString{&quot;strings&quot;}; DataPieceString description{&quot;description&quot;}; // Any string. Could be json. DataPieceStringMap&lt;Matrix4Dd&gt; aStringMatrixMap{&quot;some_string_to_matrix4d_map&quot;}; DataPieceStringMap&lt;string&gt; aStringStringMap{&quot;some_string_to_string_map&quot;}; AutoDataLayoutEnd endLayout; }; Notice that this struct must derive from AutoDataLayout, and finish with an AutoDataLayoutEnd field. This is required to make the DataLayout magic happen. Under the hood, the DataPieceXXX constructors will register themselves to the enclosing AutoDataLayout. As we will generally only create a single DataLayout instance, the overhead is minimal and does not matter. Also, notice that each field has a unique label.  ","version":"Next","tagName":"h3"},{"title":"Image, Audio, and Custom Content Blocks​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#image-audio-and-custom-content-blocks","content":" Image, audio, and custom content blocks directly contain their payload, and no additional metadata. In some cases, such as for image/jpg or image/png data, no other information is needed to interpret the data. In other cases, such with images/raw images, which are raw pixel buffers, image dimensions, pixel format and possibly stride information are required to know how to interpret the image content block. If that information never changes, it may provided directly in the RecordFormat definition, otherwise, it might need to be provided in a configuration record, or in the data records themselves, using what we call the Datalayout Conventions.  Image Content Block ExamplesAudio Content Block ExamplesCustom Content Block Examples ContentBlock(ContentType::IMAGE); // Image content block, without any detail ContentBlock(ContentType::JPG); // A jpeg image ContentBlock(ContentType::JPG, 640, 480); // A 640x480 jpeg image ContentBlock(ContentType::RAW); // A raw pixel buffer image ContentBlock(PixelFormat::GREY8, 640, 480); // A raw pixel buffer image, 640x480 large, with 8 bit greyscale pixels. Please refer to the Image Support section for more details on how to manage image content blocks.  ","version":"Next","tagName":"h3"},{"title":"Registering your RecordFormat and DataLayout Definitions​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#registering-your-recordformat-and-datalayout-definitions","content":" When you create a Recordable object to record a stream, you need to register the RecordFormat for its records. For example:  // Assuming your recordable has a member variable declared like so: MyDataLayout config_; // in your Recordable's constructor, call: addRecordFormat( Record::Type::CONFIGURATION, // record types are defined separately kConfigurationRecordFormatVersion, // only change when the RecordFormat changes config_.getContentBlock(), // RecordFormat definition: a single datalayout block {&amp;config_}); // DataLayout definition for the first block   Here is an example of a record that contains a datalayout block, followed by an image block (“datalayout+image/raw”):  // Assuming your recordable has a member variable declared like so: MyDataLayoutForDataRecords data_; // in your Recordable's constructor, call: addRecordFormat( Record::Type::DATA, // record types are defined separately kDataRecordFormatVersion, // only change when RecordFormat changes data_.getContentBlock() + ContentBlock(ImageFormat::RAW), // RecordFormat definition {&amp;data_}); // DataLayout definition for the first block, nothing for the image block   Each record has a record format version number. Each RecordFormat, its record format version number, and its DataLayout definitions are tied to a particular stream. Therefore, it is possible to have records using different RecordFormat/DataLayout definitions within a particular stream, by using different record format version numbers.  note DataLayout definitions fully describe what is stored in a datalayout content block. So, you can freely change DataLayout definitions without changing the record format version.  ","version":"Next","tagName":"h2"},{"title":"Reading Records​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#reading-records","content":" To read records described using RecordFormat conventions, attach a RecordFormatStreamPlayer to your RecordFileReader. Then, hook code to whichever of these virtual methods is appropriate for your records:  onDataLayoutRead()onImageRead()onAudioRead()onCustomBlockRead()  You will get one callback per content block, until one of the callbacks returns false, signaling that the end of the record should not be decoded.  ","version":"Next","tagName":"h2"},{"title":"Reading a Datalayout​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#reading-a-datalayout","content":" When reading a datalayout content block, you will get an onDataLayoutRead callback in your RecordFormatStreamPlayer object, with the datalayout already loaded. In the onDataLayoutRead callback, you will want to handle records differently, depending on their record type.  For each record type, you will have a specific DataLayout definition, describing the latest version of the datalayout you are using. But you cannot know if that definition matches what was read, since the file could be using an older or newer version of the datalayout definition. Use the getExpectedLayout&lt;MyDataLayout&gt; API to get a DataLayout instance of the type your code is looking for. You can then access each of its fields safely, with the caveat that each field may or may not find actual data in the datalayout that was read from disk.  Each data field is mapped according to its data type and label only. So, you do not need to worry whether fields have been added, removed, or moved. Mapping is cached per file/stream/type. So, after the first record is mapped, mapping is extremely cheap, and fields are read in constant time, no matter how complicated the datalayouts are.  tip When debugging, use DataLayout::printLayout(std::cout) to print the incoming datalayout. This will show the field names, their type, and their value, as they are in the record read.  class MyCameraStreamPlayer : public RecordFormatStreamPlayer { bool onDataLayoutRead(const CurrentRecord&amp; record, size_t blockIndex, DataLayout&amp; data) override { switch (record.recordType) { case Record::Type::CONFIGURATION: { MyCameraConfigRecordDataLayout&amp; myConfig = getExpectedLayout&lt;MyCameraConfigRecordDataLayout&gt;(data, blockIndex); // use the data... myConfig.cameraRole.get(); // access the data... } break; case Record::Type::DATA: { // Here are the fields written &amp; expected in the latest version MyCameraDataRecordDataLayout&amp; myData = getExpectedLayout&lt;MyCameraDataRecordDataLayout&gt;(data, blockIndex); // use the data... myData.cameraTemperature.get(); // Rare case: access field that were removed or renamed // e.g., frame_counter's type was changed: fetch the old version if necessary uint64_t frameCounter = 0; if (myData.frameCounter.isAvailable()) { frameCounter = myData.frameCounter.get(); } else { // MyCameraLegacyFields contains removed fields definitions MyCameraLegacyFields&amp; legacyData = getLegacyLayout&lt;MyCameraLegacyFields&gt;(data, blockIndex); frameCounter = myConversionLogic(legacyData.frameCounter.get()); } } break; default: assert(false); // should not happen, but you want to know if it does! break; } return true; // read next content blocks, if any }   ","version":"Next","tagName":"h3"},{"title":"Datalayout Conventions​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#datalayout-conventions","content":" Datalayout Conventions are a set of names and types that VRS uses to find missing RecordFormat specifications, such as the resolution and pixel format, if they are missing in the definition of an “image/raw” content block. Datalayout Conventions can also be used to specify the size of a content block when it is ambiguous. Refer to the source header &lt;vrs/DataLayoutConventions.h&gt; to see the actual Datalayout Conventions.  In the examples above, you can determine the size of the datalayout blocks by looking at the actual DataLayout definition. However, that only works if only fixed type pieces are used. When only fixed type pieces are used, the datalayout size is constant no matter what the content is. Look again at the definition of MyDataLayout above to see the difference between fixed size pieces and variable size pieces.  When only fixed size pieces are used, the getContentBlock() API generates &quot;datalayout/size=XXX&quot;, with XXX being the number of bytes. If the datalayout contains any variable size pieces, the size of the datalayout can change from record to record, and the getContentBlock() API will return &quot;datalayout&quot;.  If any variable size pieces are present, the datalayout will include an index, which has a fixed size. The index's size depends only on the number of variable size pieces declared, not on their actual values. This index makes it possible for VRS to determine the overall size of the DataLayout in two successive reads. The first read includes the data for all the fixed size pieces and the index for the variable size pieces. The added sizes found in the variable size index tells the total size of the variable size pieces, which VRS can now read with a second file read call. Therefore, VRS can always read a DataLayout block, because we can always determine its actual size.  In the second RecordFormat example above, we have a datalayout block followed by an image block (“datalayout+image/raw”). Since the image block is the last content block of the record, and VRS knows the overall size of the record, and how to figure out the size of the datalayout, we can see that all the remaining bytes must belong to the “image/raw” block. However, this is not sufficient to interpret the image pixel data. This is when we need the Datalayout Conventions.  When working with a device such as a camera, typically, during the hardware initialization/setup, before the data collection begins, the software stack will configure the camera to function in a particular mode, which includes parameters such as resolution, color mode, exposure mode, and frame rate. These parameters will never change unless the configuration of the camera is changed, which is extremely rare in practice. These parameters all belong to a configuration record and can easily be saved in a datalayout block.  In a more advanced system, a camera’s resolution and color mode may change for each frame, as when driven by a computer vision algorithm or some other heuristic. When you save only a sub-region of a whole image (the way Portal does when it tracks a target and crops the image received from the sensor), the crop size of the image might change in every frame. In such cases, the image parameters should not be placed in a configuration record. Those parameters should be specified in the datalayout block preceding the image block.  VRS uses the following heuristics:  Search each datalayout block before the ambiguous block, in the same record, in reverse content block order. If the RecordFormat is “datalayout+datalayout+image/raw+datalayout”, to disambiguate the “image/raw” block, VRS ignores the last datalayout block (because it comes after the image/raw block), and searches the second datalayout block first. If that is not enough, it then searches the first datalayout block. If the resolution and pixel format values cannot be found in the same record, VRS will search the last read configuration record in that stream.  note This look-up uses cached data. The configuration record must have been read before the data record. Reading a record will not cause another record to be read implicitly.  Using cached data works because RecordFormatStreamPlayer caches the data of the last record of each type of record it has read.  The Datalayout Conventions make RecordFormatStreamPlayer work very efficiently. For example, you get an onImageRead() callback, and the ContentBlock object is fully fleshed out, with the resolution and pixel format, which might have been specified in a configuration record a long time ago.  If VRS cannot determine, unambiguously, how the image block is formatted, it does not send an onImageRead() callback. It sends an onUnsupportedBlock() callback instead.  Datalayout Conventions can also be used to specify the size of a content block coming right after a datalayout content block. Refer to &lt;vrs/DataLayoutConventions.h&gt; for implementation details.  ","version":"Next","tagName":"h3"},{"title":"Mistakes to Avoid​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#mistakes-to-avoid","content":" While RecordFormat and DataLayout are designed to resolve a large number of backward and forward compatibility issues, you still need to be aware of the following potential problems:  Do not persist any raw struct, always copy fields one by one. If you are receiving a data structure, such as data a C struct from a driver, you need to copy each field needed in that data structure into a DataLayout, one by one, no matter how tedious. You will also have to adjust the fields in your DataLayout structure when the incoming struct changes. You might be tempted to save the whole memory block to your VRS records, to spare yourself the work. However, doing that would make your file format vulnerable to any changes made to the struct definition, which is controlled and updated by the maintainers of that data structure. It would make it extremely difficult to understand why data read from older files look corrupt, and even harder to be able to read those files. C/C++ structs do not have any data format introspection capability, and this is why VRS does not, and will not, support the use of any arbitrary &lt;class T&gt; in its template DataPiece containers. Doing so would be a massive design blunder. Writing the tedious code that copies each field, one by one, from a received data structure to a copycat DataLayout protects your data from unexpected data format changes, and will save you hours, maybe even days, of work and frustration. Do not use short, int, or size_t directly in any DataPiece template, because their size is architecture and compiler dependent, and using them can result in files that cannot be read as expected if the reading code is compiled with different configuration settings than the writing code. You should always use fully sized types, such as uint8_t instead. Do not use size_t either, because its size is not dependable. Do not persist enums using DataPieceValue&lt;ENUM_TYPE&gt; because the type of the enum is captured in the file format (int by default), and if the underlying type associated with the enum is ever changed, the data will no longer be accessible. Instead, use DataPieceEnum&lt;ENUM_TYPE, T&gt;, which captures the underlying type and performs casting. For the underlying type T, use a fixed-size integral type from &lt;cstdint&gt;, such as int32_t rather than ambiguous types like int. Be very careful when persisting external enums when casting them as integers to store them, because you might not control external enum definitions. You should convert external enums to text or to your own version of these enums. If the definition of an external enum changes, the code will start to misinterpret data, and it will be very difficult to fix. If you convert enums to text or create your own enums, whose evolution you control, you will avoid difficult debugging issues. If you really want to persist enums by casting their numeric value, you should create a unit test that will break when the enum values change, or you can simply add static_asserts to your code. Creating DataLayout objects is relatively expensive, as they use external memory buffers and indexes, but they usually do not consume too much memory. The amount of memory used is directly proportional to the number of fields in your layout, and their size. Prefer reusing a single instance of each DataLayout type that you need. Update its fields before creating the record, rather than creating a new DataLayout object on the stack each time you need to create a record. If you are using AutoDataLayout to build your DataLayout objects (like virtually everyone), be aware that their constructor uses a synchronization lock, which could potentially compromise multi-threading performance. Therefore, you really should be reusing a single instance of each DataLayout type that you need. When using containers (such as DataPieceVector and DataPieceStringMap), use stagedValues() to update the containers, rather than creating a new container each time and calling stage(). This will avoid doing a new container allocation and copy each time. For most use cases, successive records are very similar, often with an identical memory footprint, and updating containers will be significantly faster this way.  ","version":"Next","tagName":"h2"},{"title":"Additional samples​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#additional-samples","content":" You can find examples of how to create and read records using RecordFormat and DataLayout here: Datalayout sample code.  ","version":"Next","tagName":"h2"},{"title":"Why Reinvent the Wheel?​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#why-reinvent-the-wheel","content":" What's so special about DataLayout? Why did you not use JSON, Thrift, or some other existing serialized containers?  Historically, DataLayout was designed to be backward compatible with our early VRS files, which used straight-up structures of POD data. However, we now have better reasons than that. DataLayout leverages a specific pattern of sensor data collection, for which VRS was designed, where records are remarkably regular throughout a recording. For each device and record type, the exact same content blocks, using the same DataLayouts are recorded over and over again, often many millions of times.  For each record type in a stream, there is one RecordFormat, with its own set of DataLayout definitions, which is the dictionary of field types and labels in the datalayout content blocks of the stream. Each DataLayout block in the record contains only its own data, in raw binary form, which reduces processing overhead to a minimum. Therefore, the marginal cost of a DataPieceValue&lt;uint8_t&gt;, before compression, is one byte per record, regardless of its label, and even if it is the only field in the datalayout content block.  When reading and writing records, no binary-ascii conversions are made, only binary copies, and no pre or post processing of the source code is required. DataLayout definitions are as readable as possible, since they are struct definitions. The DataLayout definition is interpreted only once when the file is read, and the DataLayout that the reader expects is mapped only once to the DataLayout that is actually present in the stream. Therefore, reading the fields of a DataLayout happens in amortized constant time, with no parsing of any kind, since only pointer and size checks are required. If a field is not available in a record, the default value for that type is returned, and the isAvailable() method can be used to check.  ","version":"Next","tagName":"h2"},{"title":"What is DataLayout “really” good at?​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#what-is-datalayout-really-good-at","content":" All the power of DataLayout lies in its ability to amortize costs. Amortized, DataLayout objects...  ...store one byte of payload at the cost of 1 byte of storage (or less, because of record level compression)....have zero serialization/deserialization overhead, both on read and write, including when handling a format mismatch (that’s when the data stored in a file and the definition you have when reading that file don’t match)....have constant field access time, no matter how many you have....are pure binary containers (no string conversions, unlike json)....require no pre-processor/code generation: DataLayout definitions are directly compiled by a C++ compiler....minimize memory allocations overhead. It’s possible to create and read records without memory allocations beyond record management, even when dealing with variable size arrays (vectors). Again, amortized....look, behave, and feel like a simple C++ struct: they are very readable, very easy and efficient to read and write to....no format evolution cost: you can iterate over your DataLayout definitions as often as you like, they'll be just as efficient as if there had been no format evolutions.  The key assumption VRS makes is that data collected within each stream is extremely repetitive throughout a particular file, and everything is done to leverage that property to the fullest. DataLayout definitions are stored once per stream, parsed once per file-read, expected DataLayout formats are mapped to the DataLayout found in the stream once, so all the relatively expensive operations are amortized (done only once).  ","version":"Next","tagName":"h3"},{"title":"What is DataLayout not so good at?​","type":1,"pageTitle":"Record Format","url":"/vrs/docs/RecordFormat#what-is-datalayout-not-so-good-at","content":" seamless integration with existing data representations, like structs and other classes. You will need to write converters to copy your data source(s) to your DataLayout definitions, field by field. This can feel tedious, but ultimately, protects you against unexpected changes and provides a clear place where to handle format conversions and evolutions.Nested definitions are supported, but with limitations. See this documentation (in the “Example 2: nested definitions” tab) for details. For 99% of sensor data use cases, DataLayout works great and this limitation isn’t even apparent, but for advanced use cases with more structured data and variable formats, of when you have nested definitions with variable size data, DataLayout conversion becomes a pain point.complex data structures, in particular, arbitrary data structures that might change with every record, or not be known at compile time, so that converter code can not be written. In that case, you might need to use a self-described container, such as json or msgpack (binary variation of json). Looking at the needs of sensor data collection, this should be rare, or needed only for configuration records. Using json in configuration records is fine, because it’s typically a one record need, and the trade offs are radically different when you need to do an expensive operation and store data relatively inneficiently once during setup vs. N million times in realtime (for each record). For instance, camera calibration is often provided as a json string the stream's configuration record, and there is no reason to change that. ","version":"Next","tagName":"h3"}]